# -*- coding: utf-8 -*-
"""RNN-Word Embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K80v5vA9VgGRVkAIhkXLK-JlSK8yL3bO

##Importing Necessary Library
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

"""##Loading the Dataset"""

df_t= pd.read_csv("twitter_training.csv", sep=',', names=['Tweet_ID','Entity','Sentiment','Tweet_content'])
df_v= pd.read_csv("twitter_validation.csv", sep=',', names=['Tweet_ID','Entity','Sentiment','Tweet_content'])

df_t.head()

df_t.info()

df_v.head()

df_v.info()

"""##Data Visualization"""

# Calculate the number of sentiment labels in training data and validation data
train_sentiment_counts = df_t['Sentiment'].value_counts()
valid_sentiment_counts = df_v['Sentiment'].value_counts()

# Draw a pie chart for the training data
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.pie(train_sentiment_counts, labels=train_sentiment_counts.index, autopct='%1.1f%%', colors=['skyblue', 'lightgreen', 'lightcoral', 'orange'])
plt.title('Training Data Sentiment Distribution')

# Draw a pie chart for the valid data
plt.subplot(1, 2, 2)
plt.pie(valid_sentiment_counts, labels=valid_sentiment_counts.index, autopct='%1.1f%%', colors=['skyblue', 'lightgreen', 'lightcoral', 'orange'])
plt.title('Validation Data Sentiment Distribution')

plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.bar(train_sentiment_counts.index, train_sentiment_counts.values, color=['skyblue', 'lightgreen', 'lightcoral', 'orange'])
plt.title('Training Data Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')

# Display quantity in each column
for i, count in enumerate(train_sentiment_counts.values):
    plt.text(x=i, y=count, s=str(count), ha='center', va='bottom')


plt.subplot(1, 2, 2)
plt.bar(valid_sentiment_counts.index, valid_sentiment_counts.values, color=['skyblue', 'lightgreen', 'lightcoral', 'orange'])
plt.title('Validation Data Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')

# Display quantity in each column
for i, count in enumerate(valid_sentiment_counts.values):
    plt.text(x=i, y=count, s=str(count), ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""###Converting text data to numpy arrays and Convert labels to integers for use during training"""

# Get the column 'Tweet_content' and assign it to an array
train_sentences = df_t['Tweet_content'].to_numpy()
valid_sentences = df_v['Tweet_content'].to_numpy()

# Convert the Sentiment value to the corresponding number
label_mapping = {'Positive': 0, 'Negative': 1,'Neutral' : 2,'Irrelevant': 3,}
train_label = df_t['Sentiment'].map(label_mapping)
valid_label = df_v['Sentiment'].map(label_mapping)

print(train_sentences)
train_label

print(valid_sentences[:10])  # Show the first 10 elements of the array
valid_label

print(train_sentences[0])
train_label[0]

"""##Removing Emojis in Sentence"""

def remove_emojis(text):
    if isinstance(text, str):
        # Perform operations on the string
        return processed_text
    else:
        return text  # Return the input unchanged if it's not a string

import re

def remove_emojis(text):
    if isinstance(text, str):
        emoji_pattern = re.compile("["
                                   u"\U0001F600-\U0001F64F"  # emoticons
                                   u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                   u"\U0001F680-\U0001F6FF"  # transport & map symbols
                                   u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                                   u"\U00002500-\U00002BEF"  # chinese char
                                   u"\U00002702-\U000027B0"
                                   u"\U00002702-\U000027B0"
                                   u"\U000024C2-\U0001F251"
                                   u"\U0001f926-\U0001f937"
                                   u"\U00010000-\U0010ffff"
                                   u"\u2640-\u2642"
                                   u"\u2600-\u2B55"
                                   u"\u200d"
                                   u"\u23cf"
                                   u"\u23e9"
                                   u"\u231a"
                                   u"\ufe0f"  # dingbats
                                   u"\u3030"
                                   "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', text)
    else:
        return text

# Create an array to store sentences with emojis removed
remove_emoji_train_sentences = [remove_emojis(sentence) for sentence in train_sentences]
remove_emoji_valid_sentences = [remove_emojis(sentence) for sentence in valid_sentences]

print(remove_emoji_train_sentences[:10])
print(remove_emoji_valid_sentences[:10])

"""##Converting our labels to one-hot-encoding values"""

from sklearn.preprocessing import LabelBinarizer

#Converted our labels to one-hot-encoding values. Ex : 1 ----->[0,1,0,0,0,0,0,0,0,0]
lb = LabelBinarizer()

train_label = lb.fit_transform(train_label)
valid_label = lb.fit_transform(valid_label)

print(train_label[0])

"""##Creating Vocabulary"""

vocabulary_size = 10000

embedding_dim = 64

max_length_sentence = 150

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""###Text vectorization turns each text into a string of integers (each integer is the index of the token in the dictionary)"""

tokenizer = Tokenizer(num_words=vocabulary_size , oov_token = 'OOV', lower = True,filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')

# Filter out any non-string elements and convert them to empty strings
remove_emoji_train_sentences = [str(sentence) for sentence in remove_emoji_train_sentences]

# Fit tokenizer on the filtered sentences
tokenizer.fit_on_texts(remove_emoji_train_sentences)

# If you want to display both you can use this syntax : tokenizer.word_index
print(list(tokenizer.word_index.items())[:50])  # Display the first 10 key-value pairs

"""##Standardize train data

###Converting sentences from text to numeric strings, based on the previously created vocabulary dictionary.
"""

remove_emoji_train_sentences = tokenizer.texts_to_sequences(remove_emoji_train_sentences)

"""###The padding strings in the sentence have the same length"""

padding_train_sentences = pad_sequences(remove_emoji_train_sentences, maxlen=max_length_sentence,truncating='post',padding='post')

padding_train_sentences

padding_train_sentences.shape

"""##Standardize validation data"""

remove_emoji_valid_sentences = tokenizer.texts_to_sequences(remove_emoji_valid_sentences)
padding_valid_sentences = pad_sequences(remove_emoji_valid_sentences, maxlen=max_length_sentence,truncating='post',padding='post')

padding_valid_sentences.shape

"""##Building the Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding,Flatten,Dense

model = Sequential()

model.add(Embedding(vocabulary_size,embedding_dim,input_length=max_length_sentence))

model.add(Flatten())

model.add(Dense(10,activation='relu'))

model.add(Dense(4,activation='softmax'))

model.compile(optimizer='adam',loss="categorical_crossentropy",metrics=["accuracy"])

model.summary()

history = model.fit(padding_train_sentences,train_label,epochs=20,validation_data=(padding_valid_sentences,valid_label))

import matplotlib.pyplot as plt

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

# Plot the accuracy of training and validation
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='lower right')
plt.show()

from sklearn.metrics import accuracy_score

# Compute accuracy
accuracy = accuracy_score(valid_label, binary_predictions)
print("Accuracy:", accuracy)

from sklearn.metrics import classification_report

predictions = model.predict(padding_valid_sentences)

# Convert predictions to binary labels (0 or 1) based on a threshold (e.g., 0.5 for binary classification)
binary_predictions = (predictions > 0.5).astype(int)

# Compute and print the classification report
print(classification_report(valid_label, binary_predictions))